---
title: "STAT 361, Asg 4, Q2"
output: pdf_document
date: "2022-12-01"
---
```{r global.options, include = TRUE}
knitr::opts_chunk$set(
    cache       = TRUE,     # if TRUE knitr will cache the results to reuse in future knits
    fig.width   = 10,       # the width for plots created by code chunk
    fig.height  = 10,       # the height for plots created by code chunk
    fig.align   = 'center', # how to align graphics in the final doc. 'left', 'right', 'center'
    fig.path    = 'figs/',  # file path to the directory where knitr shall store the graphics files
    results     = 'asis',   # knitr will pass through results without reformatting them
    echo        = TRUE,     # in FALSE knitr will not display code in the code chunk above it's results
    message     = TRUE,     # if FALSE knitr will not display any messages generated by code
    strip.white = TRUE,     # if FALSE knitr will not remove white spaces at the beg or end of code chunk
    warning     = FALSE)    # if FALSE knitr will not display any warning messages in the final document
```

### Consider the dataset drug.txt. An experiment was conducted to investigate the amount of drug present in the liver of a rat. Nineteen rats were randomly selected, weighed, and placed under a light anesthetic and given an oral dose of the drug. After a fixed length of time, each rat was sacrificed and the liver weighed, and the percent dose in the liver was determined. The response Y is the proportion of the dose in the liver. The possible predictors are the body weight of each rat in grams (X1), the weight of each liver in grams (X2) and The relative dose of the drug given to each rat as a fraction of the largest dose (X3). Complete the following questions.

#### a) Fit the model and report your summary.  
```{r setup, include=FALSE}
library(tidyverse)
library(gridExtra)
library(car)

setwd("~/Desktop/STAT")
drug = read.table("drug.txt", header = TRUE)
attach(drug)
```

```{r}
myfit1 = lm(Y ~ x1 + x2 + x3)
summary(myfit1)
```

According to the linear regression, x1 and x3 are statistically significant, and overall, the linear regression is not statistically significant considering there is an overall P-value of 0.07197.

#### b) Compute the influence statistic (leverage, Cook's distance, DFFITS, DFBETAS) for each observation and identify influential points.
#### Looking at the leverage points  
```{r}
par(mfrow=c(1,3))

e1 = lm(Y ~ x1 + x2)$res
u1 = lm(x3 ~ x1 + x2)$res
plot(u1, e1, xlab = "u(1)", ylab = "e(1)")
abline(lm(e1 ~ u1)$coef, col = "red")

e2 = lm(Y ~ x1 + x3)$res
u2 = lm(x2 ~ x1 + x3)$res
plot(u2, e2, xlab = "u(2)", ylab = "e(2)")
abline(lm(e2 ~ u2)$coef, col = "red")

e3 = lm(Y ~ x2 + x3)$res
u3 = lm(x1 ~ x2 + x3)$res
plot(u3, e3, xlab = "u(3)", ylab = "e(3)")
abline(lm(e3 ~ u3)$coef, col = "red")
```

#### Identifying Influential Points
```{r}
influence.measures(myfit1)
```

#### Cook's Distance  
```{r}
par(mfrow = c(1, 3))
n = length(Y)
p = 3 
```

```{r}
plot(1:n,cooks.distance(myfit1), xlab = "observations", ylab = "cook distance values")
abline(h = qf(0.5,p+1,n-(p+1)), col = "red")
abline(h = qf(0.1,p+1,n-(p+1)), col = "red")
```

#### DFFITS  
```{r}
plot(1:n, dffits(myfit1), xlab = "observations" , ylab = "dffits values")
abline(h = 1, col = "red")
abline(h = -1, col = "red")
```

#### DFBETAS  
```{r}
plot(1:n, dfbetas(myfit1)[,1], xlab = "observations", ylab = "dfbeta values")
abline(h = 2/sqrt(n), col = "red")
abline(h = -2/sqrt(n), col = "red")
```

From Cook's Distance and DFFITS, we find points 3, and 13 to be influential. From DFBETA, we find points 5, 13, and 19 to have an impact on the dataset.

#### Outlier Test  
```{r}
outlierTest(myfit1)
```

Thus, we can determine that point 19 is not influential, but an outlier.

#### c) Refit the model with the influential point deleted and report your summary.

#### Removing Influential Points  
```{r}
drug1 = drug[-c(3, 13),]
colnames(drug1) = c("v1", "v2", "v3", "w")
attach(drug1)
```

#### Comparing the Fits of the Dataset Including and Excluding Influential Points
```{r}
summary(myfit1)
myfit2 = lm(w ~ v1 + v2 + v3)
summary(myfit2)
```

#### Visualizing the Difference Among the Datasets
```{r, echo=FALSE}
plot1 = ggplot(data = drug, aes(x = x1, y = Y)) +
  geom_point() + 
  geom_smooth(method = lm) +
  xlim(146, 200) + ylim(0, 0.75) + 
  ggtitle("Influential Points: Y vs x1")
plot2 = ggplot(data = drug, aes(x = x2, y = Y)) +
  geom_point() + 
  geom_smooth(method = lm) +
  xlim(5.2, 10) + ylim(0, 0.75) + 
  ggtitle("Influential Points: Y vs x2")
plot3 = ggplot(data = drug, aes(x = x3, y = Y)) +
  geom_point() + 
  geom_smooth(method = lm) +
  xlim(0.73, 1) + ylim(0, 0.75) + 
  ggtitle("Influential Points: Y vs x3")

plot4 = ggplot(data = drug1, aes(x = v1, y = w)) +
  geom_point() + 
  geom_smooth(method = lm) +
  xlim(146, 200) + ylim(0, 0.75) + 
  ggtitle("No Influential Points: Y vs x1")
plot5 = ggplot(data = drug1, aes(x = v2, y = w)) +
  geom_point() + 
  geom_smooth(method = lm) +
  xlim(5.2, 10) + ylim(0, 0.75) + 
  ggtitle("No Influential Points: Y vs x2")
plot6 = ggplot(data = drug1, aes(x = v3, y = w)) +
  geom_point() + 
  geom_smooth(method = lm) +
  xlim(0.73, 1) + ylim(0, 0.75) + 
  ggtitle("No Influential Points: Y vs x3")

grid.arrange(plot1, plot4, plot2, plot5, plot3, plot6)
```

#### Visualizing the Difference in the Fit of the Models  
```{r}
par(mfrow=c(2,2))
plot(myfit1, main = "Influential Points")
```

```{r}
par(mfrow=c(2,2))
plot(myfit2, main = "No Influential Points")
```
Comparing the dataset with the influential points, and the one without, we can observe that removing the influential points changes the slopes of the dataset in predictors. As well, comparing the residuals to the fitted values, we can also observe more randomization, suggesting that the relationship is more likely to be linear in the dataset without the influential points. We can also observe that in the dataset without hte influential points, the Q-Q plot is more linear, thus suggesting the data is distributed more normally.